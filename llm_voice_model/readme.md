Hereâ€™s a **fancified version** of your project description with **highlighted headings, emojis, and formatting** â€” perfect for your portfolio, GitHub, or resume:

---

## ğŸ§ ğŸ™ï¸ Voice AI Assistant (Multilingual, VAD-Based)

An intelligent **voice-enabled AI assistant** that seamlessly converts speech to text and back to speech â€” enabling **fluid, multilingual conversations**! Built using **LangChain**, **LLMs**, **Silero VAD**, **Vosk ASR**, and **Edge-TTS**, this assistant runs locally via Flask and supports English and Hindi.

---

### ğŸ”§ Key Features

* ğŸ§  **LLM Chat Integration** (LLaMA3-70B or ChatGPT via LangChain)
* ğŸ¤ **Voice Input with Voice Activity Detection** (Silero VAD)
* ğŸ—£ï¸ **Speech Recognition** via Vosk (Supports English ğŸ‡ºğŸ‡¸ & Hindi ğŸ‡®ğŸ‡³)
* ğŸ”Š **Text-to-Speech** using Microsoft Edge-TTS (Multilingual support)
* ğŸŒ **REST API** powered by Flask
* ğŸ§© LangChain RunnableChain for smooth LLM orchestration

---

### ğŸš€ How It Works

1. **ğŸ™ï¸ You speak** â€” VAD detects speech and starts/stops recording automatically.
2. **ğŸ§¾ Audio is transcribed** using Vosk (offline ASR model).
3. **ğŸ’¬ LangChain builds a prompt** and sends it to the LLM.
4. **ğŸ—£ï¸ The assistant speaks back** using Edge-TTS in the selected language.

Supports **seamless switching** between English and Hindi, with more languages easily integrable.

---

### ğŸ› ï¸ Tech Stack

* **Flask** â€“ Python web server
* **LangChain** â€“ LLM routing & orchestration
* **Silero VAD** â€“ Lightweight voice activity detection
* **Vosk ASR** â€“ Offline speech recognition (English & Hindi models)
* **Edge-TTS** â€“ Text-to-speech synthesis using Microsoft Edge voices
* **PyAudio / PyGame** â€“ Audio input/output and playback

---

### ğŸ“ Folder Structure

```
â”œâ”€â”€ app.py                        # Main Flask app
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html               # UI template
â”œâ”€â”€ vosk models/
â”‚   â”œâ”€â”€ vosk-model-small-en-us-0.15
â”‚   â””â”€â”€ vosk-model-small-hi-0.22
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ .env                         # OpenAI API key
```

---

### ğŸ“¦ Setup & Requirements

* Python 3.8+

* Install dependencies:

  ```bash
  pip install -r requirements.txt
  ```

* Add `.env` file:

  ```ini
  OPENAI_API_KEY=your_openai_key_here
  ```

* Download Vosk models from [Vosk GitHub](https://github.com/alphacep/vosk-api)

---

### ğŸ”„ API Endpoints

| Endpoint        | Method | Description                       |
| --------------- | ------ | --------------------------------- |
| `/`             | GET    | Load the UI                       |
| `/record`       | POST   | Start VAD recording + AI response |
| `/set_language` | POST   | Set current language (e.g., `en`) |
| `/get_language` | GET    | Return current language setting   |

---

### ğŸŒ Supported Languages

* âœ… English (`en`)
* âœ… Hindi (`hi`)
* ğŸ§ª Edge-TTS also supports: Urdu (`ur`), Spanish (`es`), German (`de`), Japanese (`ja`), Korean (`ko`) â€” and more.

---

### ğŸ§  Example Use Case

> **Speak** â **Transcribe with Vosk** â **Generate with LLM** â **Speak response using Edge-TTS**

---

### ğŸ’¡ Future Enhancements

* ğŸ™ï¸ Add UI microphone button for recording
* ğŸ§  Add Whisper ASR for multilingual and high-accuracy transcription
* ğŸŒ WebSocket support for **real-time streaming**
* ğŸŒ Dynamic language expansion from Edge-TTS
